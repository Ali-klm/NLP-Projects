{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGIcgNUpbYSv"
      },
      "source": [
        "# part 1 - embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSeRdsOrbjIJ"
      },
      "source": [
        "**IMDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_E8rGVnbW92"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5e5qSSGhUY5"
      },
      "outputs": [],
      "source": [
        "def train_model_with_embedding(embedding_dim, X_train, y_train, X_test, y_test):\n",
        "    max_words = 10000\n",
        "    maxlen = 50\n",
        "\n",
        "    X_train_padded = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test_padded = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "    # Create a simple model with Embedding and Dense layers for binary classification\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQcyw16hbiYZ",
        "outputId": "ced2f310-7112-4318-c222-c05fbda39db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with Embedding Dimension: 10\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 22s 66ms/step - loss: 0.5772 - accuracy: 0.6727 - val_loss: 0.4218 - val_accuracy: 0.8064\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3092 - accuracy: 0.8705 - val_loss: 0.4333 - val_accuracy: 0.7982\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1502 - accuracy: 0.9523 - val_loss: 0.5394 - val_accuracy: 0.7928\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0491 - accuracy: 0.9902 - val_loss: 0.6883 - val_accuracy: 0.7860\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0124 - accuracy: 0.9988 - val_loss: 0.7870 - val_accuracy: 0.7834\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.77944\n",
            "Precision: 0.78711\n",
            "Recall: 0.76608\n",
            "F1 Score: 0.77645\n",
            "Confusion Matrix:\n",
            "[[9910 2590]\n",
            " [2924 9576]]\n",
            "Training model with Embedding Dimension: 32\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 20s 61ms/step - loss: 0.5196 - accuracy: 0.7246 - val_loss: 0.4145 - val_accuracy: 0.8078\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.2302 - accuracy: 0.9104 - val_loss: 0.4843 - val_accuracy: 0.7916\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0466 - accuracy: 0.9902 - val_loss: 0.6319 - val_accuracy: 0.7752\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.0065 - accuracy: 0.9996 - val_loss: 0.7265 - val_accuracy: 0.7800\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7741 - val_accuracy: 0.7822\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.79380\n",
            "Precision: 0.78915\n",
            "Recall: 0.80184\n",
            "F1 Score: 0.79544\n",
            "Confusion Matrix:\n",
            "[[ 9822  2678]\n",
            " [ 2477 10023]]\n",
            "Training model with Embedding Dimension: 64\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 21s 64ms/step - loss: 0.5103 - accuracy: 0.7336 - val_loss: 0.4239 - val_accuracy: 0.7964\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 6s 21ms/step - loss: 0.1911 - accuracy: 0.9308 - val_loss: 0.5316 - val_accuracy: 0.7740\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 2s 8ms/step - loss: 0.0242 - accuracy: 0.9954 - val_loss: 0.6765 - val_accuracy: 0.7748\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 0.7287 - val_accuracy: 0.7806\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 8.8797e-04 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.7812\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.79316\n",
            "Precision: 0.79104\n",
            "Recall: 0.79680\n",
            "F1 Score: 0.79391\n",
            "Confusion Matrix:\n",
            "[[9869 2631]\n",
            " [2540 9960]]\n",
            "Training model with Embedding Dimension: 100\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 20s 62ms/step - loss: 0.5045 - accuracy: 0.7360 - val_loss: 0.4191 - val_accuracy: 0.8000\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.1726 - accuracy: 0.9391 - val_loss: 0.5483 - val_accuracy: 0.7708\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0192 - accuracy: 0.9966 - val_loss: 0.6817 - val_accuracy: 0.7780\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7395 - val_accuracy: 0.7832\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 6.6182e-04 - accuracy: 1.0000 - val_loss: 0.7762 - val_accuracy: 0.7844\n",
            "782/782 [==============================] - 1s 1ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.79412\n",
            "Precision: 0.79098\n",
            "Recall: 0.79952\n",
            "F1 Score: 0.79523\n",
            "Confusion Matrix:\n",
            "[[9859 2641]\n",
            " [2506 9994]]\n"
          ]
        }
      ],
      "source": [
        "# Load IMDB dataset with a maximum of 10000 words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "embedding_dims = [10, 32, 64, 100]\n",
        "\n",
        "# Iterate over different embedding dimensions and train models\n",
        "for embedding_dim in embedding_dims:\n",
        "    print(f\"Training model with Embedding Dimension: {embedding_dim}\")\n",
        "    train_model_with_embedding(embedding_dim, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQvMJaqsGqId"
      },
      "source": [
        "**Persian-Text-Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vwByD5Pubib3",
        "outputId": "6ff5c5e6-4fa9-47f6-de28-c26c4cec9a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "persian_dataset = load_dataset(\"SeyedAli/Persian-Text-Sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SLQ2DvRQjMmw"
      },
      "outputs": [],
      "source": [
        "def preprocess_persian_text_sentiment(dataset):\n",
        "    # Preprocess Persian Text Sentiment dataset\n",
        "    texts = dataset['train']['text']\n",
        "    labels = dataset['train']['label']\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    word_index = tokenizer.word_index\n",
        "    max_words = len(word_index) + 1\n",
        "    X = pad_sequences(sequences, maxlen=50)\n",
        "    y = np.array(labels)\n",
        "    return X, y, max_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KChHuwIeh_xS"
      },
      "outputs": [],
      "source": [
        "def train_model_with_embedding(embedding_dim, X_train, y_train, X_test, y_test, max_words):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_dim, input_length=50))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oPEg2si1s5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3144ff-0af5-464e-9dea-83fd00ffbe27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with Embedding Dimension: 10\n",
            "Epoch 1/5\n",
            "699/699 [==============================] - 37s 51ms/step - loss: 0.4185 - accuracy: 0.8024 - val_loss: 0.3557 - val_accuracy: 0.8495\n",
            "Epoch 2/5\n",
            "699/699 [==============================] - 9s 13ms/step - loss: 0.2990 - accuracy: 0.8768 - val_loss: 0.3599 - val_accuracy: 0.8467\n",
            "Epoch 3/5\n",
            "699/699 [==============================] - 5s 7ms/step - loss: 0.2369 - accuracy: 0.9082 - val_loss: 0.3929 - val_accuracy: 0.8407\n",
            "Epoch 4/5\n",
            "699/699 [==============================] - 4s 6ms/step - loss: 0.1779 - accuracy: 0.9323 - val_loss: 0.4527 - val_accuracy: 0.8243\n",
            "Epoch 5/5\n",
            "699/699 [==============================] - 6s 8ms/step - loss: 0.1313 - accuracy: 0.9521 - val_loss: 0.5342 - val_accuracy: 0.8145\n",
            "1746/1746 [==============================] - 3s 1ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.93787\n",
            "Precision: 0.92911\n",
            "Recall: 0.94808\n",
            "F1 Score: 0.93850\n",
            "Confusion Matrix:\n",
            "[[25906  2020]\n",
            " [ 1450 26476]]\n",
            "Training model with Embedding Dimension: 32\n",
            "Epoch 1/5\n",
            "699/699 [==============================] - 37s 51ms/step - loss: 0.4052 - accuracy: 0.8116 - val_loss: 0.3603 - val_accuracy: 0.8451\n",
            "Epoch 2/5\n",
            "470/699 [===================>..........] - ETA: 3s - loss: 0.2736 - accuracy: 0.8906"
          ]
        }
      ],
      "source": [
        "X_persian, y_persian, max_words_persian = preprocess_persian_text_sentiment(persian_dataset)\n",
        "\n",
        "embedding_dims = [10, 32, 64, 100]\n",
        "\n",
        "for embedding_dim in embedding_dims:\n",
        "    print(f\"Training model with Embedding Dimension: {embedding_dim}\")\n",
        "    train_model_with_embedding(embedding_dim, X_persian, y_persian, X_persian, y_persian, max_words_persian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGRRvz9H5mgD"
      },
      "source": [
        "**persian_news_dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0xqTScmdIVO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def preprocess_persian_news_dataset(dataset, maxlen=50):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for doc in dataset:\n",
        "        if len(doc['title']) > 0 and len(doc['category']) > 0:\n",
        "            texts.append(doc['text'])\n",
        "            labels.append(doc['category'])\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    word_index = tokenizer.word_index\n",
        "    max_words = len(word_index) + 1\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(labels)\n",
        "\n",
        "    return X, y, max_words\n",
        "\n",
        "dataset = load_dataset(\"saied/persian_news_dataset\", split=\"train\", streaming=True)\n",
        "\n",
        "docs = []\n",
        "document_count = 40000\n",
        "counter = 0\n",
        "skip_counter = 0\n",
        "\n",
        "for doc in dataset:\n",
        "    if len(doc['title']) == 0 or len(doc['category']) == 0:\n",
        "        skip_counter += 1\n",
        "        if skip_counter % 10000 == 0:\n",
        "            print(f'{skip_counter} skipped')\n",
        "        continue\n",
        "    docs.append(doc)\n",
        "    counter += 1\n",
        "    if counter == document_count:\n",
        "        break\n",
        "\n",
        "print(f\"Number of documents processed: {len(docs)}\")\n",
        "\n",
        "# Prepare the data\n",
        "not_proccess_x = [doc['text'] for doc in docs]\n",
        "not_proccess_y = [doc['category'] for doc in docs]\n",
        "\n",
        "# Tokenize the text data\n",
        "max_words = 10000\n",
        "max_length = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(not_proccess_x)\n",
        "\n",
        "x = tokenizer.texts_to_sequences(not_proccess_x)\n",
        "x = pad_sequences(x, maxlen=max_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(not_proccess_y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "embedding_dims = [10, 32, 64, 100]\n",
        "\n",
        "for embedding_dim in embedding_dims:\n",
        "    print(f\"Training model with Embedding Dimension: {embedding_dim}\")\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),\n",
        "        Flatten(),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(len(set(y)), activation=\"softmax\"),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    model.summary()\n",
        "    model.fit(x_train, y_train, epochs=5, batch_size=512)\n",
        "    print(\"---------------------------------------\")\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "    print(f\"test_acc: {test_acc} \\t test_loss: {test_loss}\")\n",
        "\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    unique_categories = label_encoder.classes_\n",
        "\n",
        "    for i, category in enumerate(unique_categories):\n",
        "        tp = cm[i, i]\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        tn = cm.sum() - (tp + fp + fn)\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "        accuracy = (tp + tn) / cm.sum() if cm.sum() > 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        print(\"Category:\", category)\n",
        "        print(f\"Precision: {precision:.5f}\")\n",
        "        print(f\"Recall: {recall:.5f}\")\n",
        "        print(f\"Accuracy: {accuracy:.5f}\")\n",
        "        print(f\"F1 Score: {f1_score:.5f}\")\n",
        "        print(\"--------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7oDdxf86qHf"
      },
      "source": [
        "# part 2 - pre-trained embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhzU1XLMNrwp"
      },
      "source": [
        "Step 1: Preprocess the Persian News Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ0CU2D1NFt5",
        "outputId": "c04fb7ad-7d12-450f-a276-dc271c6384d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for saied/persian_news_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/saied/persian_news_dataset\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000 skipped\n",
            "20000 skipped\n",
            "30000 skipped\n",
            "40000 skipped\n",
            "50000 skipped\n",
            "60000 skipped\n",
            "70000 skipped\n",
            "80000 skipped\n",
            "90000 skipped\n",
            "100000 skipped\n",
            "110000 skipped\n",
            "120000 skipped\n",
            "130000 skipped\n",
            "140000 skipped\n",
            "150000 skipped\n",
            "160000 skipped\n",
            "170000 skipped\n",
            "180000 skipped\n",
            "190000 skipped\n",
            "200000 skipped\n",
            "210000 skipped\n",
            "Total documents loaded: 1000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def preprocess_persian_news_dataset(dataset, maxlen=50):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for doc in dataset:\n",
        "        if len(doc['title']) > 0 and len(doc['category']) > 0:\n",
        "            texts.append(doc['text'])\n",
        "            labels.append(doc['category'])\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    word_index = tokenizer.word_index\n",
        "    max_words = len(word_index) + 1\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(labels)\n",
        "\n",
        "    return X, y, max_words, tokenizer.word_index\n",
        "\n",
        "dataset = load_dataset(\"saied/persian_news_dataset\", split=\"train\", streaming=True)\n",
        "\n",
        "docs = []\n",
        "document_count = 1000\n",
        "counter = 0\n",
        "skip_counter = 0\n",
        "\n",
        "for doc in dataset:\n",
        "    if len(doc['title']) == 0 or len(doc['category']) == 0:\n",
        "        skip_counter += 1\n",
        "        if skip_counter % 10000 == 0:\n",
        "            print(f'{skip_counter} skipped')\n",
        "        continue\n",
        "    docs.append(doc)\n",
        "    counter += 1\n",
        "    if counter == document_count:\n",
        "        break\n",
        "\n",
        "print(f\"Total documents loaded: {len(docs)}\")\n",
        "\n",
        "X, y, max_words, word_index = preprocess_persian_news_dataset(docs, maxlen=50)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNjNrb45N6am"
      },
      "source": [
        "Step 2: Create an Embedding Matrix from GloVe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lWO7sqoPViw"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "def create_embedding_matrix(word_index, max_words, embedding_dim):\n",
        "    word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "    # Initialize embedding matrix\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "    # Fill embedding matrix with pre-trained GloVe vectors\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            try:\n",
        "                embedding_vector = word_vectors[word]\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            except KeyError:\n",
        "                # Use random initialization for missing words\n",
        "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "    return embedding_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbPIH-tgOE2y"
      },
      "source": [
        "Step 3: Build and Train the Model Using the Embedding Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg_YupKbQj71",
        "outputId": "a91172ba-6b63-4cbd-e095-74d235a10d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 3s 6ms/step - loss: 0.7123 - accuracy: 0.5627 - val_loss: 0.6773 - val_accuracy: 0.5884\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.4024 - accuracy: 0.8304 - val_loss: 0.7615 - val_accuracy: 0.5920\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.1446 - accuracy: 0.9626 - val_loss: 0.9916 - val_accuracy: 0.5898\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0317 - accuracy: 0.9980 - val_loss: 1.1861 - val_accuracy: 0.5958\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.2913 - val_accuracy: 0.6036\n",
            "782/782 [==============================] - 2s 2ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.59572\n",
            "Precision: 0.59576\n",
            "Recall: 0.59572\n",
            "F1 Score: 0.59568\n",
            "Confusion Matrix:\n",
            "[[7323 5177]\n",
            " [4930 7570]]\n"
          ]
        }
      ],
      "source": [
        "def train_model_with_pretrained_embedding(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix, maxlen=50):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "train_model_with_pretrained_embedding(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6sASjCz8wxd"
      },
      "source": [
        "**IMDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y1S_hpX87eb",
        "outputId": "1deb34fc-e8db-4676-b975-3fdb713498c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6972 - accuracy: 0.5139 - val_loss: 0.6881 - val_accuracy: 0.5376\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.6765 - accuracy: 0.5695 - val_loss: 0.6788 - val_accuracy: 0.5646\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6273 - val_loss: 0.6835 - val_accuracy: 0.5690\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6571 - val_loss: 0.6977 - val_accuracy: 0.5662\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.5815 - accuracy: 0.6867 - val_loss: 0.6939 - val_accuracy: 0.5862\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.58084\n",
            "Precision: 0.58085\n",
            "Recall: 0.58084\n",
            "F1 Score: 0.58082\n",
            "Confusion Matrix:\n",
            "[[7183 5317]\n",
            " [5162 7338]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "max_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=50)\n",
        "X_test = pad_sequences(X_test, maxlen=50)\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(word_index, max_words, embedding_dim)\n",
        "\n",
        "train_model_with_pretrained_embedding(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-y3rxbWS1rH"
      },
      "source": [
        "**Persian-Text-Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k48U5J-YGhup",
        "outputId": "05a87aae-dce7-45d9-da33-bf712b792a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.7207 - accuracy: 0.5594 - val_loss: 0.6717 - val_accuracy: 0.5820\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4173 - accuracy: 0.8209 - val_loss: 0.7725 - val_accuracy: 0.5768\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1574 - accuracy: 0.9548 - val_loss: 0.9968 - val_accuracy: 0.5878\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0388 - accuracy: 0.9956 - val_loss: 1.2040 - val_accuracy: 0.5926\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0076 - accuracy: 0.9998 - val_loss: 1.3220 - val_accuracy: 0.5954\n",
            "782/782 [==============================] - 1s 2ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.59636\n",
            "Precision: 0.59642\n",
            "Recall: 0.59636\n",
            "F1 Score: 0.59630\n",
            "Confusion Matrix:\n",
            "[[7305 5195]\n",
            " [4896 7604]]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "def load_persian_text_sentiment_dataset():\n",
        "    dataset = load_dataset(\"SeyedAli/Persian-Text-Sentiment\")\n",
        "    return dataset\n",
        "\n",
        "def preprocess_persian_text_sentiment_dataset(dataset):\n",
        "    texts = [item['text'] for item in dataset['train']]\n",
        "    labels = [item['label'] for item in dataset['train']]\n",
        "    return texts, labels\n",
        "\n",
        "def create_embedding_matrix(word_index, max_words, embedding_dim):\n",
        "    word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            try:\n",
        "                embedding_vector = word_vectors[word]\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            except KeyError:\n",
        "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def train_model_with_pretrained_embedding(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix, maxlen=50):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "dataset = load_persian_text_sentiment_dataset()\n",
        "X, y = preprocess_persian_text_sentiment_dataset(dataset)\n",
        "\n",
        "max_words = 10000\n",
        "maxlen = 50\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix(tokenizer.word_index, max_words, embedding_dim)\n",
        "\n",
        "train_model_with_pretrained_embedding(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzePKhhOUdPS"
      },
      "source": [
        "**persian_news_dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"saied/persian_news_dataset\", split=\"train\", streaming=True)\n",
        "\n",
        "# Preprocess the dataset\n",
        "docs = []\n",
        "document_count = 40000\n",
        "counter = 0\n",
        "skip_counter = 0\n",
        "\n",
        "for doc in dataset:\n",
        "    if len(doc['title']) == 0 or len(doc['category']) == 0:\n",
        "        skip_counter += 1\n",
        "        if skip_counter % 10000 == 0:\n",
        "            print(f'{skip_counter} skipped')\n",
        "        continue\n",
        "    docs.append(doc)\n",
        "    counter += 1\n",
        "    if counter == document_count:\n",
        "        break\n",
        "\n",
        "print(f\"Number of documents processed: {len(docs)}\")\n",
        "\n",
        "texts = [doc['text'] for doc in docs]\n",
        "labels = [doc['category'] for doc in docs]\n",
        "\n",
        "max_words = 10000\n",
        "max_length = 50\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
        "    LSTM(32, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Flatten(),\n",
        "    Dense(10, activation=\"relu\"),\n",
        "    Dense(5, activation=\"relu\"),\n",
        "    Dense(len(set(y)), activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Build and fit the model\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"-------------------------------------\")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"test_acc: {test_acc:.5f} \\t test_loss: {test_loss:.5f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_probs = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate precision, recall, and other metrics for each class\n",
        "unique_categories = label_encoder.classes_\n",
        "\n",
        "for i, category in enumerate(unique_categories):\n",
        "    tp = cm[i, i]\n",
        "    fp = cm[:, i].sum() - tp\n",
        "    fn = cm[i, :].sum() - tp\n",
        "    tn = cm.sum() - (tp + fp + fn)\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "    accuracy = (tp + tn) / cm.sum() if cm.sum() > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"F1 Score: {f1_score:.5f}\")\n",
        "    print(\"----------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bHfHRz1l4Pf",
        "outputId": "5e44ceec-e0b0-417c-d132-4827a011d33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for saied/persian_news_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/saied/persian_news_dataset\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 skipped\n",
            "20000 skipped\n",
            "30000 skipped\n",
            "40000 skipped\n",
            "50000 skipped\n",
            "60000 skipped\n",
            "70000 skipped\n",
            "80000 skipped\n",
            "90000 skipped\n",
            "100000 skipped\n",
            "110000 skipped\n",
            "120000 skipped\n",
            "130000 skipped\n",
            "140000 skipped\n",
            "150000 skipped\n",
            "160000 skipped\n",
            "170000 skipped\n",
            "180000 skipped\n",
            "190000 skipped\n",
            "200000 skipped\n",
            "210000 skipped\n",
            "220000 skipped\n",
            "230000 skipped\n",
            "240000 skipped\n",
            "Number of documents processed: 40000\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 50, 100)           1000000   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 50, 32)            17024     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                24832     \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 5)                 55        \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 6)                 36        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1042597 (3.98 MB)\n",
            "Trainable params: 42597 (166.39 KB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "55/55 [==============================] - 22s 328ms/step - loss: 1.6801 - accuracy: 0.2714\n",
            "Epoch 2/4\n",
            "55/55 [==============================] - 23s 427ms/step - loss: 1.3336 - accuracy: 0.4274\n",
            "Epoch 3/4\n",
            "55/55 [==============================] - 22s 404ms/step - loss: 1.1106 - accuracy: 0.5188\n",
            "Epoch 4/4\n",
            "55/55 [==============================] - 19s 353ms/step - loss: 0.9386 - accuracy: 0.6160\n",
            "-------------------------------------\n",
            "375/375 [==============================] - 8s 17ms/step - loss: 0.8025 - accuracy: 0.7273\n",
            "test_acc: 0.72733 \t test_loss: 0.80253\n",
            "195/375 [==============>...............] - ETA: 4s"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 3 - simpleRNN"
      ],
      "metadata": {
        "id": "BhWyfpRW48Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMDB**"
      ],
      "metadata": {
        "id": "JrUC-8KG6YB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, SimpleRNN\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def train_model_with_embedding(embedding_dim, X_train, y_train, X_test, y_test):\n",
        "    max_words = 10000\n",
        "    maxlen = 50\n",
        "\n",
        "    X_train_padded = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test_padded = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "    model.add(SimpleRNN(32))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "(X_train_imdb, y_train_imdb), (X_test_imdb, y_test_imdb) = imdb.load_data(num_words=10000)\n",
        "\n",
        "embedding_dim_imdb = 100\n",
        "\n",
        "print(\"Training model with Embedding Dimension 100 for IMDb dataset\")\n",
        "train_model_with_embedding(embedding_dim_imdb, X_train_imdb, y_train_imdb, X_test_imdb, y_test_imdb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRXRoKAp5E4p",
        "outputId": "f7082c61-0f48-4d5f-e1f3-57352165973b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraining model with Embedding Dimension 100 for IMDb dataset\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 17s 47ms/step - loss: 0.5191 - accuracy: 0.7279 - val_loss: 0.4413 - val_accuracy: 0.7944\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.3037 - accuracy: 0.8745 - val_loss: 0.4576 - val_accuracy: 0.7886\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 12s 38ms/step - loss: 0.1260 - accuracy: 0.9559 - val_loss: 0.7816 - val_accuracy: 0.7642\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0381 - accuracy: 0.9880 - val_loss: 0.9441 - val_accuracy: 0.7742\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 1.1366 - val_accuracy: 0.7758\n",
            "782/782 [==============================] - 5s 7ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.77948\n",
            "Precision: 0.77764\n",
            "Recall: 0.78280\n",
            "F1 Score: 0.78021\n",
            "Confusion Matrix:\n",
            "[[9702 2798]\n",
            " [2715 9785]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persian Text Sentiment**"
      ],
      "metadata": {
        "id": "rqciTgNX65kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "def train_model_with_embedding_and_simplernn(embedding_dim, X_train, y_train, X_test, y_test, max_words):\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, embedding_dim, input_length=50),\n",
        "        SimpleRNN(16, return_sequences=True),  # First SimpleRNN layer with 16 units\n",
        "        SimpleRNN(32),  # Second SimpleRNN layer with 32 units\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"Training model with Embedding Dimension 100 and SimpleRNN for Persian Text Sentiment dataset\")\n",
        "train_model_with_embedding_and_simplernn(embedding_dim_persian, X_persian, y_persian, X_persian, y_persian, max_words_persian)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRcxXT6v5FOw",
        "outputId": "20828e38-ec8d-4367-dd99-db404f731909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with Embedding Dimension 100 and SimpleRNN for Persian Text Sentiment dataset\n",
            "Epoch 1/5\n",
            "699/699 [==============================] - 87s 113ms/step - loss: 0.4005 - accuracy: 0.8186 - val_loss: 0.3612 - val_accuracy: 0.8487\n",
            "Epoch 2/5\n",
            "699/699 [==============================] - 71s 102ms/step - loss: 0.2772 - accuracy: 0.8865 - val_loss: 0.3895 - val_accuracy: 0.8414\n",
            "Epoch 3/5\n",
            "699/699 [==============================] - 61s 88ms/step - loss: 0.1839 - accuracy: 0.9286 - val_loss: 0.4774 - val_accuracy: 0.8261\n",
            "Epoch 4/5\n",
            "699/699 [==============================] - 61s 87ms/step - loss: 0.1217 - accuracy: 0.9553 - val_loss: 0.5665 - val_accuracy: 0.8099\n",
            "Epoch 5/5\n",
            "699/699 [==============================] - 65s 94ms/step - loss: 0.0915 - accuracy: 0.9676 - val_loss: 0.6817 - val_accuracy: 0.8121\n",
            "1746/1746 [==============================] - 16s 9ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.94451\n",
            "Precision: 0.95882\n",
            "Recall: 0.92892\n",
            "F1 Score: 0.94364\n",
            "Confusion Matrix:\n",
            "[[26812  1114]\n",
            " [ 1985 25941]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persian News Dataset**"
      ],
      "metadata": {
        "id": "SGfo3ymd7BHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "dataset = load_dataset(\"saied/persian_news_dataset\", split=\"train\", streaming=True)\n",
        "\n",
        "docs = []\n",
        "document_count = 40000\n",
        "counter = 0\n",
        "skip_counter = 0\n",
        "\n",
        "for doc in dataset:\n",
        "    if len(doc['title']) == 0 or len(doc['category']) == 0:\n",
        "        skip_counter += 1\n",
        "        if skip_counter % 10000 == 0:\n",
        "            print(f'{skip_counter} skipped')\n",
        "        continue\n",
        "    docs.append(doc)\n",
        "    counter += 1\n",
        "    if counter == document_count:\n",
        "        break\n",
        "\n",
        "print(f\"Number of documents processed: {len(docs)}\")\n",
        "\n",
        "not_proccess_x = [doc['text'] for doc in docs]\n",
        "not_proccess_y = [doc['category'] for doc in docs]\n",
        "\n",
        "max_words = 10000\n",
        "max_length = 50\n",
        "embedding_dim = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(not_proccess_x)\n",
        "\n",
        "x = tokenizer.texts_to_sequences(not_proccess_x)\n",
        "x = pad_sequences(x, maxlen=max_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(not_proccess_y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length),\n",
        "    SimpleRNN(16, return_sequences=True),  # First RNN layer with return_sequences=True\n",
        "    SimpleRNN(32),  # Second RNN layer\n",
        "    Dense(10, activation=\"relu\"),\n",
        "    Dense(5, activation=\"relu\"),\n",
        "    Dense(len(set(y)), activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"test_acc: {test_acc:.5f} \\t test_loss: {test_loss:.5f}\")\n",
        "\n",
        "y_pred_probs = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "unique_categories = label_encoder.classes_\n",
        "\n",
        "for i, category in enumerate(unique_categories):\n",
        "    tp = cm[i, i]\n",
        "    fp = cm[:, i].sum() - tp\n",
        "    fn = cm[i, :].sum() - tp\n",
        "    tn = cm.sum() - (tp + fp + fn)\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "    accuracy = (tp + tn) / cm.sum() if cm.sum() > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1_score:.5f}\")\n",
        "    print(\"---------------------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e6c92f0fa1434192974314153eba91df",
            "b421e513b8d24afd992532c779be2720",
            "ce91e05f81b54d23ba927553bef5100a",
            "bb090b7153d649c2b514345e7dd4aeeb",
            "edb2e39b609c4d639c4025e1cf4c246f",
            "9f8f83de8a624c9db47b2dc30a6bd46a",
            "fc9e4de4cc6e40d2b4f015d075854083",
            "dc491f0afdd641fd8fe56f717c86f31b",
            "81ea6e4a17e84bacbfef9c13243e1300",
            "96b71621acd8499b82686566470ffedf",
            "d3767ba1fbb74aba886c5f8a5ebe5b7b",
            "90c711c2ac514abb8399586bdd474e63",
            "41f78f4bf64c4040bcdf6e3143c526ff",
            "e9154307e6134d468ec65f1320937200",
            "cd761b65776b4a61bdbcb9ccd91460e6",
            "9030a2ccbca543a89c236bdb407f3117",
            "588a6d2e31c34c6ba2443ce8e9bc1e64",
            "bc4e3224fe3f45c2941bec122b1bc3e8",
            "34656826d504438f800a1b2b62151995",
            "3a256e0c9a384ba784927d0c6a3ab87f",
            "c439fb4c1849429e800ecf41b179d7d3",
            "15969cc6a6b54479ae36050029b48d13",
            "b1cb83ea0dc84de3b819585ed99788da",
            "0098936f20bd49c79796cbc07fa119e5",
            "25ced070e5264b60bbbdd2fb02a2e913",
            "02cc6badebb24e15a6a635e3d7942df8",
            "6b7026486d9d42a8912e6aad6ee43dbe",
            "04282e72ad3f41d3ba781aaea8480155",
            "5b9c424405bb4c958be55f1fd109a961",
            "f6e7fb46c2ba4399adcd8ebbdf322829",
            "01803eb285834f6896887b186aac5cda",
            "72d8ec48ffb441f68f861e92f4e48f30",
            "6c91f680f69844a3bf3bd116a6fe7b4b"
          ]
        },
        "collapsed": true,
        "id": "NbgiAdKA7FEi",
        "outputId": "151b2b6b-e7df-4d1e-bcfa-ff915d7611df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for saied/persian_news_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/saied/persian_news_dataset\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6c92f0fa1434192974314153eba91df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90c711c2ac514abb8399586bdd474e63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/5.35k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1cb83ea0dc84de3b819585ed99788da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 skipped\n",
            "20000 skipped\n",
            "30000 skipped\n",
            "40000 skipped\n",
            "50000 skipped\n",
            "60000 skipped\n",
            "70000 skipped\n",
            "80000 skipped\n",
            "90000 skipped\n",
            "100000 skipped\n",
            "110000 skipped\n",
            "120000 skipped\n",
            "130000 skipped\n",
            "140000 skipped\n",
            "150000 skipped\n",
            "160000 skipped\n",
            "170000 skipped\n",
            "180000 skipped\n",
            "190000 skipped\n",
            "200000 skipped\n",
            "210000 skipped\n",
            "220000 skipped\n",
            "230000 skipped\n",
            "240000 skipped\n",
            "Number of documents processed: 40000\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 50, 100)           1000000   \n",
            "                                                                 \n",
            " simple_rnn_6 (SimpleRNN)    (None, 50, 16)            1872      \n",
            "                                                                 \n",
            " simple_rnn_7 (SimpleRNN)    (None, 32)                1568      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 5)                 55        \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 6)                 36        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1003861 (3.83 MB)\n",
            "Trainable params: 1003861 (3.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "55/55 [==============================] - 6s 71ms/step - loss: 1.3888 - accuracy: 0.3469\n",
            "Epoch 2/4\n",
            "55/55 [==============================] - 5s 97ms/step - loss: 0.9117 - accuracy: 0.6166\n",
            "Epoch 3/4\n",
            "55/55 [==============================] - 5s 83ms/step - loss: 0.5793 - accuracy: 0.7907\n",
            "Epoch 4/4\n",
            "55/55 [==============================] - 4s 76ms/step - loss: 0.3380 - accuracy: 0.8681\n",
            "-------------------------------------\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.2482 - accuracy: 0.9961\n",
            "test_acc: 0.99608 \t test_loss: 0.24818\n",
            "375/375 [==============================] - 3s 7ms/step\n",
            "Confusion Matrix:\n",
            "[[2397    0    1    0    0    0]\n",
            " [   0 2654    0    0    0    0]\n",
            " [  24    0 2135    0    0    0]\n",
            " [   0    0    0 2368    0    0]\n",
            " [   7    0    0    0 1246    0]\n",
            " [  13    0    0    0    2 1153]]\n",
            "Category: \n",
            "Accuracy: 0.99625\n",
            "Precision: 0.98197\n",
            "Recall: 0.99958\n",
            "F1 Score: 0.99070\n",
            "---------------------------------------------------\n",
            "Category: \n",
            "Accuracy: 1.00000\n",
            "Precision: 1.00000\n",
            "Recall: 1.00000\n",
            "F1 Score: 1.00000\n",
            "---------------------------------------------------\n",
            "Category: \n",
            "Accuracy: 0.99792\n",
            "Precision: 0.99953\n",
            "Recall: 0.98888\n",
            "F1 Score: 0.99418\n",
            "---------------------------------------------------\n",
            "Category: \n",
            "Accuracy: 1.00000\n",
            "Precision: 1.00000\n",
            "Recall: 1.00000\n",
            "F1 Score: 1.00000\n",
            "---------------------------------------------------\n",
            "Category: \n",
            "Accuracy: 0.99925\n",
            "Precision: 0.99840\n",
            "Recall: 0.99441\n",
            "F1 Score: 0.99640\n",
            "---------------------------------------------------\n",
            "Category: \n",
            "Accuracy: 0.99875\n",
            "Precision: 1.00000\n",
            "Recall: 0.98716\n",
            "F1 Score: 0.99354\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part 4 - LSTM"
      ],
      "metadata": {
        "id": "Fq6uCrUnC04y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**imdb**"
      ],
      "metadata": {
        "id": "3mA4Cnw2ED5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense, LSTM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "max_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=50)\n",
        "X_test = pad_sequences(X_test, maxlen=50)\n",
        "\n",
        "def create_embedding_matrix(word_index, max_words, embedding_dim):\n",
        "    word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            try:\n",
        "                embedding_vector = word_vectors[word]\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            except KeyError:\n",
        "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix(word_index, max_words, embedding_dim)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=50, weights=[embedding_matrix], trainable=False),\n",
        "    LSTM(32, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"test_acc: {test_acc:.5f} \\t test_loss: {test_loss:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGez_aqaEw3c",
        "outputId": "f49f99be-2c56-42b8-bf0e-3172a13749bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 32s 80ms/step - loss: 0.6768 - accuracy: 0.5550 - val_loss: 0.6443 - val_accuracy: 0.6218\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 24s 78ms/step - loss: 0.6186 - accuracy: 0.6446 - val_loss: 0.6220 - val_accuracy: 0.6352\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 24s 76ms/step - loss: 0.5815 - accuracy: 0.6773 - val_loss: 0.5756 - val_accuracy: 0.6736\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 26s 84ms/step - loss: 0.5471 - accuracy: 0.7062 - val_loss: 0.5567 - val_accuracy: 0.6952\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5184 - accuracy: 0.7268 - val_loss: 0.5531 - val_accuracy: 0.7092\n",
            "-------------------------------------\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.5415 - accuracy: 0.7224\n",
            "test_acc: 0.72244 \t test_loss: 0.54155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persian Text Sentiment**"
      ],
      "metadata": {
        "id": "yH6Cu-I2Fstf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "def load_persian_text_sentiment_dataset():\n",
        "    dataset = load_dataset(\"SeyedAli/Persian-Text-Sentiment\")\n",
        "    return dataset\n",
        "\n",
        "def preprocess_persian_text_sentiment_dataset(dataset):\n",
        "    texts = [item['text'] for item in dataset['train']]\n",
        "    labels = [item['label'] for item in dataset['train']]\n",
        "    return texts, labels\n",
        "\n",
        "def create_embedding_matrix(word_index, max_words, embedding_dim):\n",
        "    word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i < max_words:\n",
        "            try:\n",
        "                embedding_vector = word_vectors[word]\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            except KeyError:\n",
        "                embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "def train_model_with_lstm(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix, maxlen=50):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False))\n",
        "    model.add(LSTM(32, return_sequences=True))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"F1 Score: {f1:.5f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "dataset = load_persian_text_sentiment_dataset()\n",
        "X, y = preprocess_persian_text_sentiment_dataset(dataset)\n",
        "\n",
        "max_words = 10000\n",
        "maxlen = 50\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = create_embedding_matrix(tokenizer.word_index, max_words, embedding_dim)\n",
        "\n",
        "train_model_with_lstm(embedding_dim, X_train, y_train, X_test, y_test, max_words, embedding_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py7H78szF9vk",
        "outputId": "f50b748b-174b-4ffe-9d03-fd01fe930e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 31s 85ms/step - loss: 0.6169 - accuracy: 0.6454 - val_loss: 0.5565 - val_accuracy: 0.6934\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 24s 75ms/step - loss: 0.4941 - accuracy: 0.7572 - val_loss: 0.5234 - val_accuracy: 0.7390\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 24s 76ms/step - loss: 0.4335 - accuracy: 0.7975 - val_loss: 0.4811 - val_accuracy: 0.7638\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.3849 - accuracy: 0.8245 - val_loss: 0.4762 - val_accuracy: 0.7632\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.3540 - accuracy: 0.8421 - val_loss: 0.4808 - val_accuracy: 0.7640\n",
            "782/782 [==============================] - 17s 19ms/step\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.77156\n",
            "Precision: 0.77271\n",
            "Recall: 0.77156\n",
            "F1 Score: 0.77132\n",
            "Confusion Matrix:\n",
            "[[10050  2450]\n",
            " [ 3261  9239]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Persian News Dataset**"
      ],
      "metadata": {
        "id": "RpDKxJuAGVC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"saied/persian_news_dataset\", split=\"train\", streaming=True)\n",
        "\n",
        "# Preprocess the dataset\n",
        "docs = []\n",
        "document_count = 40000\n",
        "counter = 0\n",
        "skip_counter = 0\n",
        "\n",
        "for doc in dataset:\n",
        "    if len(doc['title']) == 0 or len(doc['category']) == 0:\n",
        "        skip_counter += 1\n",
        "        if skip_counter % 10000 == 0:\n",
        "            print(f'{skip_counter} skipped')\n",
        "        continue\n",
        "    docs.append(doc)\n",
        "    counter += 1\n",
        "    if counter == document_count:\n",
        "        break\n",
        "\n",
        "print(f\"Number of documents processed: {len(docs)}\")\n",
        "\n",
        "texts = [doc['text'] for doc in docs]\n",
        "labels = [doc['category'] for doc in docs]\n",
        "\n",
        "max_words = 10000\n",
        "max_length = 50\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Load GloVe embeddings using gensim\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words:\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
        "    LSTM(32, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Flatten(),\n",
        "    Dense(10, activation=\"relu\"),\n",
        "    Dense(5, activation=\"relu\"),\n",
        "    Dense(len(set(y)), activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Build and fit the model\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"-------------------------------------\")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"test_acc: {test_acc:.5f} \\t test_loss: {test_loss:.5f}\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_probs = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate precision, recall, and other metrics for each class\n",
        "unique_categories = label_encoder.classes_\n",
        "\n",
        "for i, category in enumerate(unique_categories):\n",
        "    tp = cm[i, i]\n",
        "    fp = cm[:, i].sum() - tp\n",
        "    fn = cm[i, :].sum() - tp\n",
        "    tn = cm.sum() - (tp + fp + fn)\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "    accuracy = (tp + tn) / cm.sum() if cm.sum() > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"Precision: {precision:.5f}\")\n",
        "    print(f\"Recall: {recall:.5f}\")\n",
        "    print(f\"Accuracy: {accuracy:.5f}\")\n",
        "    print(f\"F1 Score: {f1_score:.5f}\")\n",
        "    print(\"----------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wsd8AGrZGcnN",
        "outputId": "e97f8e57-7fb7-4f32-bf38-85245751890c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for saied/persian_news_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/saied/persian_news_dataset\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 skipped\n",
            "20000 skipped\n",
            "30000 skipped\n",
            "40000 skipped\n",
            "50000 skipped\n",
            "60000 skipped\n",
            "70000 skipped\n",
            "80000 skipped\n",
            "90000 skipped\n",
            "100000 skipped\n",
            "110000 skipped\n",
            "120000 skipped\n",
            "130000 skipped\n",
            "140000 skipped\n",
            "150000 skipped\n",
            "160000 skipped\n",
            "170000 skipped\n",
            "180000 skipped\n",
            "190000 skipped\n",
            "200000 skipped\n",
            "210000 skipped\n",
            "220000 skipped\n",
            "230000 skipped\n",
            "240000 skipped\n",
            "Number of documents processed: 40000\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 50, 100)           1000000   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 50, 32)            17024     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 64)                24832     \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 5)                 55        \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 6)                 36        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1042597 (3.98 MB)\n",
            "Trainable params: 42597 (166.39 KB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "55/55 [==============================] - 34s 537ms/step - loss: 1.7534 - accuracy: 0.2522\n",
            "Epoch 2/4\n",
            "55/55 [==============================] - 20s 357ms/step - loss: 1.4885 - accuracy: 0.3954\n",
            "Epoch 3/4\n",
            "55/55 [==============================] - 20s 366ms/step - loss: 0.9894 - accuracy: 0.6680\n",
            "Epoch 4/4\n",
            "55/55 [==============================] - 20s 352ms/step - loss: 0.6174 - accuracy: 0.8107\n",
            "-------------------------------------\n",
            "375/375 [==============================] - 9s 22ms/step - loss: 0.4411 - accuracy: 0.8707\n",
            "test_acc: 0.87067 \t test_loss: 0.44106\n",
            "375/375 [==============================] - 8s 20ms/step\n",
            "Confusion Matrix:\n",
            "[[2101   32   74    1  190    0]\n",
            " [  50 2356  240    0    2    6]\n",
            " [  50   16 2091    0    0    2]\n",
            " [  10    0   28 2324    1    5]\n",
            " [ 685    3   18    1  545    1]\n",
            " [  39   28   26   42    2 1031]]\n",
            "Category: \n",
            "Precision: 0.71584\n",
            "Recall: 0.87615\n",
            "Accuracy: 0.90575\n",
            "F1 Score: 0.78792\n",
            "----------------------------------------\n",
            "Category: \n",
            "Precision: 0.96756\n",
            "Recall: 0.88772\n",
            "Accuracy: 0.96858\n",
            "F1 Score: 0.92592\n",
            "----------------------------------------\n",
            "Category: \n",
            "Precision: 0.84417\n",
            "Recall: 0.96850\n",
            "Accuracy: 0.96217\n",
            "F1 Score: 0.90207\n",
            "----------------------------------------\n",
            "Category: \n",
            "Precision: 0.98142\n",
            "Recall: 0.98142\n",
            "Accuracy: 0.99267\n",
            "F1 Score: 0.98142\n",
            "----------------------------------------\n",
            "Category: \n",
            "Precision: 0.73649\n",
            "Recall: 0.43496\n",
            "Accuracy: 0.92475\n",
            "F1 Score: 0.54691\n",
            "----------------------------------------\n",
            "Category: \n",
            "Precision: 0.98660\n",
            "Recall: 0.88271\n",
            "Accuracy: 0.98742\n",
            "F1 Score: 0.93177\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e6c92f0fa1434192974314153eba91df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b421e513b8d24afd992532c779be2720",
              "IPY_MODEL_ce91e05f81b54d23ba927553bef5100a",
              "IPY_MODEL_bb090b7153d649c2b514345e7dd4aeeb"
            ],
            "layout": "IPY_MODEL_edb2e39b609c4d639c4025e1cf4c246f"
          }
        },
        "b421e513b8d24afd992532c779be2720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f8f83de8a624c9db47b2dc30a6bd46a",
            "placeholder": "",
            "style": "IPY_MODEL_fc9e4de4cc6e40d2b4f015d075854083",
            "value": "Downloadingbuilderscript:100%"
          }
        },
        "ce91e05f81b54d23ba927553bef5100a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc491f0afdd641fd8fe56f717c86f31b",
            "max": 1841,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81ea6e4a17e84bacbfef9c13243e1300",
            "value": 1841
          }
        },
        "bb090b7153d649c2b514345e7dd4aeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b71621acd8499b82686566470ffedf",
            "placeholder": "",
            "style": "IPY_MODEL_d3767ba1fbb74aba886c5f8a5ebe5b7b",
            "value": "1.84k/1.84k[00:00&lt;00:00,70.8kB/s]"
          }
        },
        "edb2e39b609c4d639c4025e1cf4c246f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8f83de8a624c9db47b2dc30a6bd46a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc9e4de4cc6e40d2b4f015d075854083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc491f0afdd641fd8fe56f717c86f31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ea6e4a17e84bacbfef9c13243e1300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96b71621acd8499b82686566470ffedf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3767ba1fbb74aba886c5f8a5ebe5b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90c711c2ac514abb8399586bdd474e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41f78f4bf64c4040bcdf6e3143c526ff",
              "IPY_MODEL_e9154307e6134d468ec65f1320937200",
              "IPY_MODEL_cd761b65776b4a61bdbcb9ccd91460e6"
            ],
            "layout": "IPY_MODEL_9030a2ccbca543a89c236bdb407f3117"
          }
        },
        "41f78f4bf64c4040bcdf6e3143c526ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588a6d2e31c34c6ba2443ce8e9bc1e64",
            "placeholder": "",
            "style": "IPY_MODEL_bc4e3224fe3f45c2941bec122b1bc3e8",
            "value": "Downloadingmetadata:100%"
          }
        },
        "e9154307e6134d468ec65f1320937200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34656826d504438f800a1b2b62151995",
            "max": 1254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a256e0c9a384ba784927d0c6a3ab87f",
            "value": 1254
          }
        },
        "cd761b65776b4a61bdbcb9ccd91460e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c439fb4c1849429e800ecf41b179d7d3",
            "placeholder": "",
            "style": "IPY_MODEL_15969cc6a6b54479ae36050029b48d13",
            "value": "1.25k/1.25k[00:00&lt;00:00,49.4kB/s]"
          }
        },
        "9030a2ccbca543a89c236bdb407f3117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588a6d2e31c34c6ba2443ce8e9bc1e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc4e3224fe3f45c2941bec122b1bc3e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34656826d504438f800a1b2b62151995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a256e0c9a384ba784927d0c6a3ab87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c439fb4c1849429e800ecf41b179d7d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15969cc6a6b54479ae36050029b48d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1cb83ea0dc84de3b819585ed99788da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0098936f20bd49c79796cbc07fa119e5",
              "IPY_MODEL_25ced070e5264b60bbbdd2fb02a2e913",
              "IPY_MODEL_02cc6badebb24e15a6a635e3d7942df8"
            ],
            "layout": "IPY_MODEL_6b7026486d9d42a8912e6aad6ee43dbe"
          }
        },
        "0098936f20bd49c79796cbc07fa119e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04282e72ad3f41d3ba781aaea8480155",
            "placeholder": "",
            "style": "IPY_MODEL_5b9c424405bb4c958be55f1fd109a961",
            "value": "Downloadingreadme:100%"
          }
        },
        "25ced070e5264b60bbbdd2fb02a2e913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e7fb46c2ba4399adcd8ebbdf322829",
            "max": 5347,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01803eb285834f6896887b186aac5cda",
            "value": 5347
          }
        },
        "02cc6badebb24e15a6a635e3d7942df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d8ec48ffb441f68f861e92f4e48f30",
            "placeholder": "",
            "style": "IPY_MODEL_6c91f680f69844a3bf3bd116a6fe7b4b",
            "value": "5.35k/5.35k[00:00&lt;00:00,257kB/s]"
          }
        },
        "6b7026486d9d42a8912e6aad6ee43dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04282e72ad3f41d3ba781aaea8480155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9c424405bb4c958be55f1fd109a961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6e7fb46c2ba4399adcd8ebbdf322829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01803eb285834f6896887b186aac5cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72d8ec48ffb441f68f861e92f4e48f30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c91f680f69844a3bf3bd116a6fe7b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}